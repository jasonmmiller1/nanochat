2025-11-30 21:00:33,911 - nanochat.checkpoint_manager - [32m[1mINFO[0m - No model tag provided, guessing model tag: d20
2025-11-30 21:00:33,912 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Loading model from /home/ubuntu/.cache/nanochat/mid_checkpoints/d20 with step 808
2025-11-30 21:00:35,162 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}
Target examples per step: 32
Device batch size: 4
Examples per step is device_batch_size * ddp_world_size: 32
=> Setting grad accum steps: 1
Scaling the LR for the AdamW parameters ‚àù1/‚àö(1280/768) = 0.774597
Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32
Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32
Step 00000 | Validation loss: 1.017535
Step 00000/00701 | Training loss: 0.977048| lrm: 1.000000| num_tokens: 10,466
Step 00001/00701 | Training loss: 0.865718| lrm: 0.998573| num_tokens: 13,599
Step 00002/00701 | Training loss: 1.149698| lrm: 0.997147| num_tokens: 10,160
Step 00003/00701 | Training loss: 1.117614| lrm: 0.995720| num_tokens: 14,356
Step 00004/00701 | Training loss: 0.690585| lrm: 0.994294| num_tokens: 11,037
Step 00005/00701 | Training loss: 0.756910| lrm: 0.992867| num_tokens: 8,097
Step 00006/00701 | Training loss: 0.901470| lrm: 0.991441| num_tokens: 11,158
Step 00007/00701 | Training loss: 1.168492| lrm: 0.990014| num_tokens: 12,208
Step 00008/00701 | Training loss: 0.883497| lrm: 0.988588| num_tokens: 9,369
Step 00009/00701 | Training loss: 0.728059| lrm: 0.987161| num_tokens: 11,171
Step 00010/00701 | Training loss: 0.450231| lrm: 0.985735| num_tokens: 16,690
Step 00011/00701 | Training loss: 0.911776| lrm: 0.984308| num_tokens: 11,585
Step 00012/00701 | Training loss: 0.769801| lrm: 0.982882| num_tokens: 11,801
Step 00013/00701 | Training loss: 0.967406| lrm: 0.981455| num_tokens: 11,683
Step 00014/00701 | Training loss: 0.395526| lrm: 0.980029| num_tokens: 7,199
Step 00015/00701 | Training loss: 0.976793| lrm: 0.978602| num_tokens: 11,956
Step 00016/00701 | Training loss: 0.839524| lrm: 0.977175| num_tokens: 17,538
Step 00017/00701 | Training loss: 0.970338| lrm: 0.975749| num_tokens: 12,526
Step 00018/00701 | Training loss: 1.080777| lrm: 0.974322| num_tokens: 15,100
Step 00019/00701 | Training loss: 0.572606| lrm: 0.972896| num_tokens: 9,716
Step 00020/00701 | Training loss: 0.547005| lrm: 0.971469| num_tokens: 13,638
Step 00021/00701 | Training loss: 0.485028| lrm: 0.970043| num_tokens: 6,738
Step 00022/00701 | Training loss: 1.125919| lrm: 0.968616| num_tokens: 7,966
Step 00023/00701 | Training loss: 1.047317| lrm: 0.967190| num_tokens: 10,094
Step 00024/00701 | Training loss: 0.831564| lrm: 0.965763| num_tokens: 13,821
Step 00025/00701 | Training loss: 0.810575| lrm: 0.964337| num_tokens: 7,556
Step 00026/00701 | Training loss: 0.998671| lrm: 0.962910| num_tokens: 14,477
Step 00027/00701 | Training loss: 1.082464| lrm: 0.961484| num_tokens: 9,886
Step 00028/00701 | Training loss: 0.917053| lrm: 0.960057| num_tokens: 10,523
Step 00029/00701 | Training loss: 0.989736| lrm: 0.958631| num_tokens: 8,140
Step 00030/00701 | Training loss: 0.544304| lrm: 0.957204| num_tokens: 8,756
Step 00031/00701 | Training loss: 1.020299| lrm: 0.955777| num_tokens: 8,324
Step 00032/00701 | Training loss: 0.309433| lrm: 0.954351| num_tokens: 11,618
Step 00033/00701 | Training loss: 1.019743| lrm: 0.952924| num_tokens: 10,073
Step 00034/00701 | Training loss: 1.007686| lrm: 0.951498| num_tokens: 14,307
Step 00035/00701 | Training loss: 1.034129| lrm: 0.950071| num_tokens: 11,875
Step 00036/00701 | Training loss: 1.059692| lrm: 0.948645| num_tokens: 9,823
Step 00037/00701 | Training loss: 0.511191| lrm: 0.947218| num_tokens: 12,349
Step 00038/00701 | Training loss: 0.880463| lrm: 0.945792| num_tokens: 8,328
Step 00039/00701 | Training loss: 0.878954| lrm: 0.944365| num_tokens: 8,622
Step 00040/00701 | Training loss: 0.978183| lrm: 0.942939| num_tokens: 15,544
Step 00041/00701 | Training loss: 1.237799| lrm: 0.941512| num_tokens: 12,547
Step 00042/00701 | Training loss: 0.937037| lrm: 0.940086| num_tokens: 12,274
Step 00043/00701 | Training loss: 0.952264| lrm: 0.938659| num_tokens: 11,637
Step 00044/00701 | Training loss: 0.538463| lrm: 0.937233| num_tokens: 11,490
Step 00045/00701 | Training loss: 0.684140| lrm: 0.935806| num_tokens: 12,258
Step 00046/00701 | Training loss: 0.826512| lrm: 0.934379| num_tokens: 11,002
Step 00047/00701 | Training loss: 0.776354| lrm: 0.932953| num_tokens: 11,301
Step 00048/00701 | Training loss: 0.896607| lrm: 0.931526| num_tokens: 9,923
Step 00049/00701 | Training loss: 0.666339| lrm: 0.930100| num_tokens: 11,712
Step 00050/00701 | Training loss: 1.122093| lrm: 0.928673| num_tokens: 9,944
Step 00051/00701 | Training loss: 0.826038| lrm: 0.927247| num_tokens: 12,630
Step 00052/00701 | Training loss: 0.729164| lrm: 0.925820| num_tokens: 8,603
Step 00053/00701 | Training loss: 0.524463| lrm: 0.924394| num_tokens: 9,674
Step 00054/00701 | Training loss: 0.953616| lrm: 0.922967| num_tokens: 12,712
Step 00055/00701 | Training loss: 0.834483| lrm: 0.921541| num_tokens: 6,118
Step 00056/00701 | Training loss: 1.058319| lrm: 0.920114| num_tokens: 8,461
Step 00057/00701 | Training loss: 0.820374| lrm: 0.918688| num_tokens: 13,417
Step 00058/00701 | Training loss: 1.013297| lrm: 0.917261| num_tokens: 14,365
Step 00059/00701 | Training loss: 0.512328| lrm: 0.915835| num_tokens: 8,186
Step 00060/00701 | Training loss: 0.988813| lrm: 0.914408| num_tokens: 13,109
Step 00061/00701 | Training loss: 1.072876| lrm: 0.912981| num_tokens: 12,710
Step 00062/00701 | Training loss: 0.504715| lrm: 0.911555| num_tokens: 8,199
Step 00063/00701 | Training loss: 1.156117| lrm: 0.910128| num_tokens: 11,091
Step 00064/00701 | Training loss: 1.050624| lrm: 0.908702| num_tokens: 9,875
Step 00065/00701 | Training loss: 0.953919| lrm: 0.907275| num_tokens: 12,029
Step 00066/00701 | Training loss: 0.594018| lrm: 0.905849| num_tokens: 11,693
Step 00067/00701 | Training loss: 0.772935| lrm: 0.904422| num_tokens: 11,039
Step 00068/00701 | Training loss: 0.948782| lrm: 0.902996| num_tokens: 11,382
Step 00069/00701 | Training loss: 0.582745| lrm: 0.901569| num_tokens: 4,919
Step 00070/00701 | Training loss: 1.091673| lrm: 0.900143| num_tokens: 14,507
Step 00071/00701 | Training loss: 0.841009| lrm: 0.898716| num_tokens: 11,636
Step 00072/00701 | Training loss: 0.985835| lrm: 0.897290| num_tokens: 9,415
Step 00073/00701 | Training loss: 1.108066| lrm: 0.895863| num_tokens: 8,844
Step 00074/00701 | Training loss: 0.730592| lrm: 0.894437| num_tokens: 9,889
Step 00075/00701 | Training loss: 0.826108| lrm: 0.893010| num_tokens: 6,609
Step 00076/00701 | Training loss: 0.827888| lrm: 0.891583| num_tokens: 12,984
Step 00077/00701 | Training loss: 0.880870| lrm: 0.890157| num_tokens: 13,015
Step 00078/00701 | Training loss: 0.698923| lrm: 0.888730| num_tokens: 12,963
Step 00079/00701 | Training loss: 0.884846| lrm: 0.887304| num_tokens: 6,960
Step 00080/00701 | Training loss: 0.784621| lrm: 0.885877| num_tokens: 9,063
Step 00081/00701 | Training loss: 0.473185| lrm: 0.884451| num_tokens: 7,024
Step 00082/00701 | Training loss: 1.079714| lrm: 0.883024| num_tokens: 14,610
Step 00083/00701 | Training loss: 0.442578| lrm: 0.881598| num_tokens: 12,657
Step 00084/00701 | Training loss: 0.942645| lrm: 0.880171| num_tokens: 12,175
Step 00085/00701 | Training loss: 0.789529| lrm: 0.878745| num_tokens: 8,595
Step 00086/00701 | Training loss: 1.158023| lrm: 0.877318| num_tokens: 11,350
Step 00087/00701 | Training loss: 0.424484| lrm: 0.875892| num_tokens: 7,029
Step 00088/00701 | Training loss: 0.740027| lrm: 0.874465| num_tokens: 13,055
Step 00089/00701 | Training loss: 1.064677| lrm: 0.873039| num_tokens: 11,051
Step 00090/00701 | Training loss: 0.940022| lrm: 0.871612| num_tokens: 11,056
Step 00091/00701 | Training loss: 1.309339| lrm: 0.870185| num_tokens: 7,161
Step 00092/00701 | Training loss: 0.780743| lrm: 0.868759| num_tokens: 11,618
Step 00093/00701 | Training loss: 0.772996| lrm: 0.867332| num_tokens: 12,280
Step 00094/00701 | Training loss: 0.937222| lrm: 0.865906| num_tokens: 9,249
Step 00095/00701 | Training loss: 0.997855| lrm: 0.864479| num_tokens: 10,196
Step 00096/00701 | Training loss: 1.142546| lrm: 0.863053| num_tokens: 14,123
Step 00097/00701 | Training loss: 0.889716| lrm: 0.861626| num_tokens: 12,603
Step 00098/00701 | Training loss: 1.013087| lrm: 0.860200| num_tokens: 12,997
Step 00099/00701 | Training loss: 1.188815| lrm: 0.858773| num_tokens: 13,339
Step 00100 | Validation loss: 1.020333
Step 00100/00701 | Training loss: 0.604705| lrm: 0.857347| num_tokens: 11,938
Step 00101/00701 | Training loss: 0.407773| lrm: 0.855920| num_tokens: 8,506
Step 00102/00701 | Training loss: 0.784192| lrm: 0.854494| num_tokens: 11,003
Step 00103/00701 | Training loss: 0.728344| lrm: 0.853067| num_tokens: 14,339
Step 00104/00701 | Training loss: 0.880692| lrm: 0.851641| num_tokens: 6,429
Step 00105/00701 | Training loss: 0.900235| lrm: 0.850214| num_tokens: 9,222
Step 00106/00701 | Training loss: 1.009517| lrm: 0.848787| num_tokens: 14,672
Step 00107/00701 | Training loss: 0.677317| lrm: 0.847361| num_tokens: 11,178
Step 00108/00701 | Training loss: 0.960424| lrm: 0.845934| num_tokens: 9,748
Step 00109/00701 | Training loss: 1.021526| lrm: 0.844508| num_tokens: 11,002
Step 00110/00701 | Training loss: 1.070513| lrm: 0.843081| num_tokens: 10,291
Step 00111/00701 | Training loss: 1.057285| lrm: 0.841655| num_tokens: 9,379
Step 00112/00701 | Training loss: 0.574571| lrm: 0.840228| num_tokens: 11,342
Step 00113/00701 | Training loss: 0.900583| lrm: 0.838802| num_tokens: 14,188
Step 00114/00701 | Training loss: 0.989058| lrm: 0.837375| num_tokens: 12,612
Step 00115/00701 | Training loss: 1.009973| lrm: 0.835949| num_tokens: 9,056
Step 00116/00701 | Training loss: 1.036815| lrm: 0.834522| num_tokens: 8,162
Step 00117/00701 | Training loss: 1.016720| lrm: 0.833096| num_tokens: 15,741
Step 00118/00701 | Training loss: 1.197427| lrm: 0.831669| num_tokens: 5,807
Step 00119/00701 | Training loss: 0.949648| lrm: 0.830243| num_tokens: 9,885
Step 00120/00701 | Training loss: 1.067590| lrm: 0.828816| num_tokens: 10,081
Step 00121/00701 | Training loss: 1.062872| lrm: 0.827389| num_tokens: 16,162
Step 00122/00701 | Training loss: 0.569108| lrm: 0.825963| num_tokens: 6,968
Step 00123/00701 | Training loss: 0.916390| lrm: 0.824536| num_tokens: 9,783
Step 00124/00701 | Training loss: 0.746861| lrm: 0.823110| num_tokens: 11,613
Step 00125/00701 | Training loss: 0.661295| lrm: 0.821683| num_tokens: 12,532
Step 00126/00701 | Training loss: 0.579401| lrm: 0.820257| num_tokens: 11,807
Step 00127/00701 | Training loss: 0.657946| lrm: 0.818830| num_tokens: 11,245
Step 00128/00701 | Training loss: 0.854218| lrm: 0.817404| num_tokens: 11,312
Step 00129/00701 | Training loss: 0.578916| lrm: 0.815977| num_tokens: 13,431
Step 00130/00701 | Training loss: 0.804118| lrm: 0.814551| num_tokens: 11,309
Step 00131/00701 | Training loss: 1.120292| lrm: 0.813124| num_tokens: 9,919
Step 00132/00701 | Training loss: 1.316297| lrm: 0.811698| num_tokens: 13,859
Step 00133/00701 | Training loss: 1.073231| lrm: 0.810271| num_tokens: 11,348
Step 00134/00701 | Training loss: 0.815817| lrm: 0.808845| num_tokens: 7,057
Step 00135/00701 | Training loss: 1.173839| lrm: 0.807418| num_tokens: 11,275
Step 00136/00701 | Training loss: 0.832403| lrm: 0.805991| num_tokens: 10,820
Step 00137/00701 | Training loss: 0.694218| lrm: 0.804565| num_tokens: 11,418
Step 00138/00701 | Training loss: 0.527120| lrm: 0.803138| num_tokens: 7,503
Step 00139/00701 | Training loss: 0.629812| lrm: 0.801712| num_tokens: 10,295
Step 00140/00701 | Training loss: 1.053001| lrm: 0.800285| num_tokens: 14,119
Step 00141/00701 | Training loss: 0.886027| lrm: 0.798859| num_tokens: 14,052
Step 00142/00701 | Training loss: 0.574238| lrm: 0.797432| num_tokens: 10,095
Step 00143/00701 | Training loss: 0.614608| lrm: 0.796006| num_tokens: 9,082
Step 00144/00701 | Training loss: 0.479246| lrm: 0.794579| num_tokens: 12,123
Step 00145/00701 | Training loss: 0.280281| lrm: 0.793153| num_tokens: 10,356
Step 00146/00701 | Training loss: 0.538647| lrm: 0.791726| num_tokens: 11,792
Step 00147/00701 | Training loss: 0.754456| lrm: 0.790300| num_tokens: 11,406
Step 00148/00701 | Training loss: 0.986214| lrm: 0.788873| num_tokens: 11,851
Step 00149/00701 | Training loss: 1.036615| lrm: 0.787447| num_tokens: 10,965
Step 00150/00701 | Training loss: 0.925731| lrm: 0.786020| num_tokens: 12,442
Step 00151/00701 | Training loss: 0.882465| lrm: 0.784593| num_tokens: 13,097
Step 00152/00701 | Training loss: 0.846232| lrm: 0.783167| num_tokens: 9,273
Step 00153/00701 | Training loss: 0.579999| lrm: 0.781740| num_tokens: 9,340
Step 00154/00701 | Training loss: 0.893027| lrm: 0.780314| num_tokens: 10,309
Step 00155/00701 | Training loss: 1.115998| lrm: 0.778887| num_tokens: 11,642
Step 00156/00701 | Training loss: 0.548338| lrm: 0.777461| num_tokens: 9,852
Step 00157/00701 | Training loss: 0.645856| lrm: 0.776034| num_tokens: 11,823
Step 00158/00701 | Training loss: 1.349207| lrm: 0.774608| num_tokens: 9,889
Step 00159/00701 | Training loss: 0.804055| lrm: 0.773181| num_tokens: 8,294
Step 00160/00701 | Training loss: 0.944379| lrm: 0.771755| num_tokens: 9,147
Step 00161/00701 | Training loss: 0.917362| lrm: 0.770328| num_tokens: 10,368
Step 00162/00701 | Training loss: 0.788372| lrm: 0.768902| num_tokens: 11,818
Step 00163/00701 | Training loss: 0.811497| lrm: 0.767475| num_tokens: 12,740
Step 00164/00701 | Training loss: 0.797239| lrm: 0.766049| num_tokens: 10,021
Step 00165/00701 | Training loss: 0.747252| lrm: 0.764622| num_tokens: 10,501
Step 00166/00701 | Training loss: 1.000627| lrm: 0.763195| num_tokens: 10,713
Step 00167/00701 | Training loss: 0.849433| lrm: 0.761769| num_tokens: 11,109
Step 00168/00701 | Training loss: 0.485740| lrm: 0.760342| num_tokens: 7,203
Step 00169/00701 | Training loss: 0.970013| lrm: 0.758916| num_tokens: 8,279
Step 00170/00701 | Training loss: 0.905431| lrm: 0.757489| num_tokens: 10,397
Step 00171/00701 | Training loss: 0.728565| lrm: 0.756063| num_tokens: 9,095
Step 00172/00701 | Training loss: 1.335540| lrm: 0.754636| num_tokens: 10,190
Step 00173/00701 | Training loss: 0.683539| lrm: 0.753210| num_tokens: 11,924
Step 00174/00701 | Training loss: 0.356483| lrm: 0.751783| num_tokens: 9,396
Step 00175/00701 | Training loss: 0.941850| lrm: 0.750357| num_tokens: 14,366
Step 00176/00701 | Training loss: 0.782132| lrm: 0.748930| num_tokens: 13,229
Step 00177/00701 | Training loss: 0.730709| lrm: 0.747504| num_tokens: 10,423
Step 00178/00701 | Training loss: 0.527988| lrm: 0.746077| num_tokens: 5,404
Step 00179/00701 | Training loss: 0.648601| lrm: 0.744650| num_tokens: 9,709
Step 00180/00701 | Training loss: 0.559278| lrm: 0.743224| num_tokens: 12,616
Step 00181/00701 | Training loss: 1.185847| lrm: 0.741797| num_tokens: 14,208
Step 00182/00701 | Training loss: 1.323289| lrm: 0.740371| num_tokens: 11,936
Step 00183/00701 | Training loss: 0.499772| lrm: 0.738944| num_tokens: 10,579
Step 00184/00701 | Training loss: 0.969882| lrm: 0.737518| num_tokens: 12,366
Step 00185/00701 | Training loss: 0.677818| lrm: 0.736091| num_tokens: 9,865
Step 00186/00701 | Training loss: 0.754312| lrm: 0.734665| num_tokens: 10,017
Step 00187/00701 | Training loss: 0.762371| lrm: 0.733238| num_tokens: 10,321
Step 00188/00701 | Training loss: 0.662002| lrm: 0.731812| num_tokens: 7,022
Step 00189/00701 | Training loss: 1.434614| lrm: 0.730385| num_tokens: 6,412
Step 00190/00701 | Training loss: 0.724244| lrm: 0.728959| num_tokens: 12,971
Step 00191/00701 | Training loss: 1.010228| lrm: 0.727532| num_tokens: 11,477
Step 00192/00701 | Training loss: 0.455770| lrm: 0.726106| num_tokens: 11,435
Step 00193/00701 | Training loss: 0.920062| lrm: 0.724679| num_tokens: 8,983
Step 00194/00701 | Training loss: 0.994560| lrm: 0.723252| num_tokens: 10,248
Step 00195/00701 | Training loss: 0.815266| lrm: 0.721826| num_tokens: 14,030
Step 00196/00701 | Training loss: 0.823272| lrm: 0.720399| num_tokens: 10,726
Step 00197/00701 | Training loss: 1.309622| lrm: 0.718973| num_tokens: 6,880
Step 00198/00701 | Training loss: 0.816065| lrm: 0.717546| num_tokens: 11,226
Step 00199/00701 | Training loss: 0.799968| lrm: 0.716120| num_tokens: 10,491
Step 00200 | Validation loss: 1.020308
Final: 357/1024 (34.86%)
Final: 436/1024 (42.58%)
Step 00200 | mmlu_acc: 0.348633, arc_easy_acc: 0.425781
Step 00200/00701 | Training loss: 1.068192| lrm: 0.714693| num_tokens: 14,806
Step 00201/00701 | Training loss: 1.170804| lrm: 0.713267| num_tokens: 11,085
Step 00202/00701 | Training loss: 0.333986| lrm: 0.711840| num_tokens: 9,086
Step 00203/00701 | Training loss: 0.662222| lrm: 0.710414| num_tokens: 10,164
Step 00204/00701 | Training loss: 0.617888| lrm: 0.708987| num_tokens: 10,171
Step 00205/00701 | Training loss: 1.087716| lrm: 0.707561| num_tokens: 16,669
Step 00206/00701 | Training loss: 1.354477| lrm: 0.706134| num_tokens: 10,099
Step 00207/00701 | Training loss: 0.890480| lrm: 0.704708| num_tokens: 10,332
Step 00208/00701 | Training loss: 0.848668| lrm: 0.703281| num_tokens: 10,368
Step 00209/00701 | Training loss: 0.910824| lrm: 0.701854| num_tokens: 9,765
Step 00210/00701 | Training loss: 1.034827| lrm: 0.700428| num_tokens: 8,631
Step 00211/00701 | Training loss: 0.717518| lrm: 0.699001| num_tokens: 11,115
Step 00212/00701 | Training loss: 0.803691| lrm: 0.697575| num_tokens: 13,312
Step 00213/00701 | Training loss: 0.847403| lrm: 0.696148| num_tokens: 10,326
Step 00214/00701 | Training loss: 0.880762| lrm: 0.694722| num_tokens: 10,216
Step 00215/00701 | Training loss: 0.463749| lrm: 0.693295| num_tokens: 6,102
Step 00216/00701 | Training loss: 0.746657| lrm: 0.691869| num_tokens: 8,157
Step 00217/00701 | Training loss: 1.003361| lrm: 0.690442| num_tokens: 5,878
Step 00218/00701 | Training loss: 0.593327| lrm: 0.689016| num_tokens: 14,537
Step 00219/00701 | Training loss: 0.968814| lrm: 0.687589| num_tokens: 12,707
Step 00220/00701 | Training loss: 0.776447| lrm: 0.686163| num_tokens: 6,965
Step 00221/00701 | Training loss: 0.852662| lrm: 0.684736| num_tokens: 7,754
Step 00222/00701 | Training loss: 0.849521| lrm: 0.683310| num_tokens: 9,965
Step 00223/00701 | Training loss: 1.181760| lrm: 0.681883| num_tokens: 11,908
Step 00224/00701 | Training loss: 0.723979| lrm: 0.680456| num_tokens: 10,353
Step 00225/00701 | Training loss: 0.694984| lrm: 0.679030| num_tokens: 10,192
Step 00226/00701 | Training loss: 0.832386| lrm: 0.677603| num_tokens: 9,429
Step 00227/00701 | Training loss: 0.675943| lrm: 0.676177| num_tokens: 7,339
Step 00228/00701 | Training loss: 1.253879| lrm: 0.674750| num_tokens: 12,459
Step 00229/00701 | Training loss: 1.056469| lrm: 0.673324| num_tokens: 12,192
Step 00230/00701 | Training loss: 1.001329| lrm: 0.671897| num_tokens: 7,195
Step 00231/00701 | Training loss: 0.622846| lrm: 0.670471| num_tokens: 10,077
Step 00232/00701 | Training loss: 0.705842| lrm: 0.669044| num_tokens: 12,782
Step 00233/00701 | Training loss: 0.706951| lrm: 0.667618| num_tokens: 10,038
Step 00234/00701 | Training loss: 0.795947| lrm: 0.666191| num_tokens: 16,279
Step 00235/00701 | Training loss: 1.016307| lrm: 0.664765| num_tokens: 7,830
Step 00236/00701 | Training loss: 1.030053| lrm: 0.663338| num_tokens: 10,039
Step 00237/00701 | Training loss: 0.847788| lrm: 0.661912| num_tokens: 10,706
Step 00238/00701 | Training loss: 1.163379| lrm: 0.660485| num_tokens: 9,674
Step 00239/00701 | Training loss: 0.536022| lrm: 0.659058| num_tokens: 12,972
Step 00240/00701 | Training loss: 0.740893| lrm: 0.657632| num_tokens: 10,259
Step 00241/00701 | Training loss: 0.635489| lrm: 0.656205| num_tokens: 6,886
Step 00242/00701 | Training loss: 0.760145| lrm: 0.654779| num_tokens: 8,388
Step 00243/00701 | Training loss: 0.527354| lrm: 0.653352| num_tokens: 12,877
Step 00244/00701 | Training loss: 1.106987| lrm: 0.651926| num_tokens: 5,897
Step 00245/00701 | Training loss: 0.884721| lrm: 0.650499| num_tokens: 14,761
Step 00246/00701 | Training loss: 0.892850| lrm: 0.649073| num_tokens: 12,519
Step 00247/00701 | Training loss: 1.015291| lrm: 0.647646| num_tokens: 13,574
Step 00248/00701 | Training loss: 0.511653| lrm: 0.646220| num_tokens: 13,498
Step 00249/00701 | Training loss: 0.768122| lrm: 0.644793| num_tokens: 10,793
Step 00250/00701 | Training loss: 1.095394| lrm: 0.643367| num_tokens: 10,290
Step 00251/00701 | Training loss: 1.157508| lrm: 0.641940| num_tokens: 11,885
Step 00252/00701 | Training loss: 0.663844| lrm: 0.640514| num_tokens: 9,768
Step 00253/00701 | Training loss: 1.051223| lrm: 0.639087| num_tokens: 12,856
Step 00254/00701 | Training loss: 0.751788| lrm: 0.637660| num_tokens: 7,510
Step 00255/00701 | Training loss: 0.792880| lrm: 0.636234| num_tokens: 11,461
Step 00256/00701 | Training loss: 0.620882| lrm: 0.634807| num_tokens: 8,585
Step 00257/00701 | Training loss: 0.748556| lrm: 0.633381| num_tokens: 9,115
Step 00258/00701 | Training loss: 1.403658| lrm: 0.631954| num_tokens: 13,411
Step 00259/00701 | Training loss: 1.052316| lrm: 0.630528| num_tokens: 7,494
Step 00260/00701 | Training loss: 1.089897| lrm: 0.629101| num_tokens: 13,317
Step 00261/00701 | Training loss: 0.701114| lrm: 0.627675| num_tokens: 10,294
Step 00262/00701 | Training loss: 1.285668| lrm: 0.626248| num_tokens: 10,487
Step 00263/00701 | Training loss: 0.823743| lrm: 0.624822| num_tokens: 7,590
Step 00264/00701 | Training loss: 0.689441| lrm: 0.623395| num_tokens: 9,603
Step 00265/00701 | Training loss: 0.948502| lrm: 0.621969| num_tokens: 9,412
Step 00266/00701 | Training loss: 0.587350| lrm: 0.620542| num_tokens: 12,040
Step 00267/00701 | Training loss: 1.296070| lrm: 0.619116| num_tokens: 12,256
Step 00268/00701 | Training loss: 1.227357| lrm: 0.617689| num_tokens: 12,658
Step 00269/00701 | Training loss: 0.793317| lrm: 0.616262| num_tokens: 10,003
Step 00270/00701 | Training loss: 1.148252| lrm: 0.614836| num_tokens: 12,206
Step 00271/00701 | Training loss: 0.792972| lrm: 0.613409| num_tokens: 8,234
Step 00272/00701 | Training loss: 0.863199| lrm: 0.611983| num_tokens: 11,311
Step 00273/00701 | Training loss: 0.570054| lrm: 0.610556| num_tokens: 11,324
Step 00274/00701 | Training loss: 0.997329| lrm: 0.609130| num_tokens: 13,040
Step 00275/00701 | Training loss: 0.930158| lrm: 0.607703| num_tokens: 8,297
Step 00276/00701 | Training loss: 0.729493| lrm: 0.606277| num_tokens: 8,290
Step 00277/00701 | Training loss: 0.930430| lrm: 0.604850| num_tokens: 9,004
Step 00278/00701 | Training loss: 0.597213| lrm: 0.603424| num_tokens: 7,888
Step 00279/00701 | Training loss: 0.394042| lrm: 0.601997| num_tokens: 9,914
Step 00280/00701 | Training loss: 1.097758| lrm: 0.600571| num_tokens: 10,168
Step 00281/00701 | Training loss: 1.063443| lrm: 0.599144| num_tokens: 13,931
Step 00282/00701 | Training loss: 1.105519| lrm: 0.597718| num_tokens: 14,559
Step 00283/00701 | Training loss: 0.742762| lrm: 0.596291| num_tokens: 10,084
Step 00284/00701 | Training loss: 1.002087| lrm: 0.594864| num_tokens: 11,478
Step 00285/00701 | Training loss: 0.854689| lrm: 0.593438| num_tokens: 11,091
Step 00286/00701 | Training loss: 0.879268| lrm: 0.592011| num_tokens: 14,494
Step 00287/00701 | Training loss: 0.635470| lrm: 0.590585| num_tokens: 12,235
Step 00288/00701 | Training loss: 0.780078| lrm: 0.589158| num_tokens: 8,446
Step 00289/00701 | Training loss: 0.931466| lrm: 0.587732| num_tokens: 10,720
Step 00290/00701 | Training loss: 1.281353| lrm: 0.586305| num_tokens: 9,027
Step 00291/00701 | Training loss: 0.645647| lrm: 0.584879| num_tokens: 12,274
Step 00292/00701 | Training loss: 0.771280| lrm: 0.583452| num_tokens: 15,445
Step 00293/00701 | Training loss: 1.083093| lrm: 0.582026| num_tokens: 12,951
Step 00294/00701 | Training loss: 0.664376| lrm: 0.580599| num_tokens: 10,600
Step 00295/00701 | Training loss: 0.894119| lrm: 0.579173| num_tokens: 8,165
Step 00296/00701 | Training loss: 1.108464| lrm: 0.577746| num_tokens: 11,591
Step 00297/00701 | Training loss: 0.910437| lrm: 0.576320| num_tokens: 15,290
Step 00298/00701 | Training loss: 0.720689| lrm: 0.574893| num_tokens: 10,497
Step 00299/00701 | Training loss: 0.607894| lrm: 0.573466| num_tokens: 9,916
Step 00300 | Validation loss: 1.020422
Step 00300/00701 | Training loss: 0.639860| lrm: 0.572040| num_tokens: 7,525
Step 00301/00701 | Training loss: 0.667073| lrm: 0.570613| num_tokens: 9,463
Step 00302/00701 | Training loss: 0.776224| lrm: 0.569187| num_tokens: 11,117
Step 00303/00701 | Training loss: 0.872966| lrm: 0.567760| num_tokens: 11,072
Step 00304/00701 | Training loss: 1.083408| lrm: 0.566334| num_tokens: 11,471
Step 00305/00701 | Training loss: 0.568373| lrm: 0.564907| num_tokens: 12,302
Step 00306/00701 | Training loss: 0.791550| lrm: 0.563481| num_tokens: 13,218
Step 00307/00701 | Training loss: 0.668295| lrm: 0.562054| num_tokens: 13,048
Step 00308/00701 | Training loss: 0.348194| lrm: 0.560628| num_tokens: 10,063
Step 00309/00701 | Training loss: 1.239287| lrm: 0.559201| num_tokens: 9,718
Step 00310/00701 | Training loss: 0.792033| lrm: 0.557775| num_tokens: 12,452
Step 00311/00701 | Training loss: 1.008082| lrm: 0.556348| num_tokens: 12,204
Step 00312/00701 | Training loss: 0.891462| lrm: 0.554922| num_tokens: 10,592
Step 00313/00701 | Training loss: 0.673068| lrm: 0.553495| num_tokens: 8,178
Step 00314/00701 | Training loss: 0.632671| lrm: 0.552068| num_tokens: 7,751
Step 00315/00701 | Training loss: 0.658896| lrm: 0.550642| num_tokens: 13,173
Step 00316/00701 | Training loss: 0.440808| lrm: 0.549215| num_tokens: 7,857
Step 00317/00701 | Training loss: 0.749435| lrm: 0.547789| num_tokens: 11,350
Step 00318/00701 | Training loss: 0.749146| lrm: 0.546362| num_tokens: 11,751
Step 00319/00701 | Training loss: 0.993150| lrm: 0.544936| num_tokens: 8,674
Step 00320/00701 | Training loss: 0.614267| lrm: 0.543509| num_tokens: 8,225
Step 00321/00701 | Training loss: 0.948489| lrm: 0.542083| num_tokens: 7,745
Step 00322/00701 | Training loss: 0.715985| lrm: 0.540656| num_tokens: 7,832
Step 00323/00701 | Training loss: 0.837813| lrm: 0.539230| num_tokens: 13,154
Step 00324/00701 | Training loss: 1.294660| lrm: 0.537803| num_tokens: 14,323
Step 00325/00701 | Training loss: 0.933475| lrm: 0.536377| num_tokens: 7,804
Step 00326/00701 | Training loss: 1.143037| lrm: 0.534950| num_tokens: 8,045
Step 00327/00701 | Training loss: 0.630299| lrm: 0.533524| num_tokens: 14,199
Step 00328/00701 | Training loss: 0.960761| lrm: 0.532097| num_tokens: 11,079
Step 00329/00701 | Training loss: 0.664470| lrm: 0.530670| num_tokens: 11,362
Step 00330/00701 | Training loss: 1.017337| lrm: 0.529244| num_tokens: 10,385
Step 00331/00701 | Training loss: 1.191464| lrm: 0.527817| num_tokens: 9,262
Step 00332/00701 | Training loss: 1.952509| lrm: 0.526391| num_tokens: 10,580
Step 00333/00701 | Training loss: 1.085081| lrm: 0.524964| num_tokens: 10,710
Step 00334/00701 | Training loss: 0.779792| lrm: 0.523538| num_tokens: 9,683
Step 00335/00701 | Training loss: 0.470359| lrm: 0.522111| num_tokens: 10,669
Step 00336/00701 | Training loss: 0.990177| lrm: 0.520685| num_tokens: 11,950
Step 00337/00701 | Training loss: 1.515499| lrm: 0.519258| num_tokens: 10,911
Step 00338/00701 | Training loss: 0.840476| lrm: 0.517832| num_tokens: 10,739
Step 00339/00701 | Training loss: 0.825673| lrm: 0.516405| num_tokens: 13,806
Step 00340/00701 | Training loss: 0.918193| lrm: 0.514979| num_tokens: 11,655
Step 00341/00701 | Training loss: 1.010776| lrm: 0.513552| num_tokens: 10,638
Step 00342/00701 | Training loss: 1.027588| lrm: 0.512126| num_tokens: 8,065
Step 00343/00701 | Training loss: 0.731401| lrm: 0.510699| num_tokens: 10,630
Step 00344/00701 | Training loss: 0.969524| lrm: 0.509272| num_tokens: 10,197
Step 00345/00701 | Training loss: 0.512570| lrm: 0.507846| num_tokens: 9,726
Step 00346/00701 | Training loss: 0.658076| lrm: 0.506419| num_tokens: 11,547
Step 00347/00701 | Training loss: 0.862911| lrm: 0.504993| num_tokens: 10,531
Step 00348/00701 | Training loss: 0.693594| lrm: 0.503566| num_tokens: 8,946
Step 00349/00701 | Training loss: 1.099778| lrm: 0.502140| num_tokens: 10,892
Step 00350/00701 | Training loss: 0.945791| lrm: 0.500713| num_tokens: 12,818
Step 00351/00701 | Training loss: 0.973775| lrm: 0.499287| num_tokens: 10,938
Step 00352/00701 | Training loss: 0.834248| lrm: 0.497860| num_tokens: 11,945
Step 00353/00701 | Training loss: 1.106628| lrm: 0.496434| num_tokens: 9,544
Step 00354/00701 | Training loss: 1.061473| lrm: 0.495007| num_tokens: 12,396
Step 00355/00701 | Training loss: 0.817568| lrm: 0.493581| num_tokens: 9,696
Step 00356/00701 | Training loss: 0.833042| lrm: 0.492154| num_tokens: 9,147
Step 00357/00701 | Training loss: 1.269603| lrm: 0.490728| num_tokens: 12,398
Step 00358/00701 | Training loss: 1.065010| lrm: 0.489301| num_tokens: 11,475
Step 00359/00701 | Training loss: 0.761532| lrm: 0.487874| num_tokens: 11,750
Step 00360/00701 | Training loss: 0.622270| lrm: 0.486448| num_tokens: 13,156
Step 00361/00701 | Training loss: 1.000287| lrm: 0.485021| num_tokens: 11,056
Step 00362/00701 | Training loss: 0.623294| lrm: 0.483595| num_tokens: 12,130
Step 00363/00701 | Training loss: 0.940260| lrm: 0.482168| num_tokens: 11,480
Step 00364/00701 | Training loss: 0.559469| lrm: 0.480742| num_tokens: 11,501
Step 00365/00701 | Training loss: 0.803657| lrm: 0.479315| num_tokens: 11,658
Step 00366/00701 | Training loss: 0.991087| lrm: 0.477889| num_tokens: 7,911
Step 00367/00701 | Training loss: 1.066059| lrm: 0.476462| num_tokens: 13,241
Step 00368/00701 | Training loss: 0.596182| lrm: 0.475036| num_tokens: 12,300
Step 00369/00701 | Training loss: 0.408014| lrm: 0.473609| num_tokens: 12,428
Step 00370/00701 | Training loss: 0.892743| lrm: 0.472183| num_tokens: 17,440
Step 00371/00701 | Training loss: 0.537505| lrm: 0.470756| num_tokens: 13,343
Step 00372/00701 | Training loss: 0.906555| lrm: 0.469330| num_tokens: 9,212
Step 00373/00701 | Training loss: 1.218205| lrm: 0.467903| num_tokens: 9,378
Step 00374/00701 | Training loss: 1.100389| lrm: 0.466476| num_tokens: 12,523
Step 00375/00701 | Training loss: 1.135340| lrm: 0.465050| num_tokens: 15,430
Step 00376/00701 | Training loss: 0.818281| lrm: 0.463623| num_tokens: 14,198
Step 00377/00701 | Training loss: 0.772292| lrm: 0.462197| num_tokens: 14,514
Step 00378/00701 | Training loss: 1.023233| lrm: 0.460770| num_tokens: 10,497
Step 00379/00701 | Training loss: 0.737672| lrm: 0.459344| num_tokens: 7,803
Step 00380/00701 | Training loss: 0.958863| lrm: 0.457917| num_tokens: 9,150
Step 00381/00701 | Training loss: 0.342845| lrm: 0.456491| num_tokens: 15,692
Step 00382/00701 | Training loss: 0.938358| lrm: 0.455064| num_tokens: 12,396
Step 00383/00701 | Training loss: 0.766454| lrm: 0.453638| num_tokens: 9,577
Step 00384/00701 | Training loss: 0.812663| lrm: 0.452211| num_tokens: 12,944
Step 00385/00701 | Training loss: 0.789595| lrm: 0.450785| num_tokens: 8,029
Step 00386/00701 | Training loss: 0.716197| lrm: 0.449358| num_tokens: 10,897
Step 00387/00701 | Training loss: 1.059325| lrm: 0.447932| num_tokens: 12,757
Step 00388/00701 | Training loss: 1.049857| lrm: 0.446505| num_tokens: 13,442
Step 00389/00701 | Training loss: 1.230458| lrm: 0.445078| num_tokens: 13,137
Step 00390/00701 | Training loss: 1.012351| lrm: 0.443652| num_tokens: 11,563
Step 00391/00701 | Training loss: 0.398930| lrm: 0.442225| num_tokens: 12,031
Step 00392/00701 | Training loss: 0.735088| lrm: 0.440799| num_tokens: 12,239
Step 00393/00701 | Training loss: 0.223530| lrm: 0.439372| num_tokens: 11,065
Step 00394/00701 | Training loss: 1.329240| lrm: 0.437946| num_tokens: 9,785
Step 00395/00701 | Training loss: 1.113219| lrm: 0.436519| num_tokens: 11,940
Step 00396/00701 | Training loss: 0.533893| lrm: 0.435093| num_tokens: 7,975
Step 00397/00701 | Training loss: 0.881528| lrm: 0.433666| num_tokens: 14,033
Step 00398/00701 | Training loss: 0.527652| lrm: 0.432240| num_tokens: 7,098
Step 00399/00701 | Training loss: 1.165415| lrm: 0.430813| num_tokens: 8,522
Step 00400 | Validation loss: 1.019963
Final: 342/1024 (33.40%)
Final: 439/1024 (42.87%)
Step 00400 | mmlu_acc: 0.333984, arc_easy_acc: 0.428711
Step 00400/00701 | Training loss: 0.929266| lrm: 0.429387| num_tokens: 10,404
Step 00401/00701 | Training loss: 0.624116| lrm: 0.427960| num_tokens: 8,800
Step 00402/00701 | Training loss: 0.923580| lrm: 0.426534| num_tokens: 9,421
Step 00403/00701 | Training loss: 0.754981| lrm: 0.425107| num_tokens: 14,241
Step 00404/00701 | Training loss: 0.403357| lrm: 0.423680| num_tokens: 8,029
Step 00405/00701 | Training loss: 0.716391| lrm: 0.422254| num_tokens: 10,413
Step 00406/00701 | Training loss: 0.723639| lrm: 0.420827| num_tokens: 14,284
Step 00407/00701 | Training loss: 0.872162| lrm: 0.419401| num_tokens: 6,919
Step 00408/00701 | Training loss: 0.610925| lrm: 0.417974| num_tokens: 12,915
Step 00409/00701 | Training loss: 0.755910| lrm: 0.416548| num_tokens: 8,116
Step 00410/00701 | Training loss: 1.209954| lrm: 0.415121| num_tokens: 14,426
Step 00411/00701 | Training loss: 0.830220| lrm: 0.413695| num_tokens: 11,541
Step 00412/00701 | Training loss: 1.133052| lrm: 0.412268| num_tokens: 8,210
Step 00413/00701 | Training loss: 0.592418| lrm: 0.410842| num_tokens: 8,365
Step 00414/00701 | Training loss: 1.016638| lrm: 0.409415| num_tokens: 10,991
Step 00415/00701 | Training loss: 1.003088| lrm: 0.407989| num_tokens: 13,785
Step 00416/00701 | Training loss: 0.844017| lrm: 0.406562| num_tokens: 12,485
Step 00417/00701 | Training loss: 0.466279| lrm: 0.405136| num_tokens: 11,607
Step 00418/00701 | Training loss: 0.837030| lrm: 0.403709| num_tokens: 6,441
Step 00419/00701 | Training loss: 0.743861| lrm: 0.402282| num_tokens: 12,884
Step 00420/00701 | Training loss: 0.900594| lrm: 0.400856| num_tokens: 9,922
Step 00421/00701 | Training loss: 0.648246| lrm: 0.399429| num_tokens: 10,202
Step 00422/00701 | Training loss: 0.936661| lrm: 0.398003| num_tokens: 14,408
Step 00423/00701 | Training loss: 0.673033| lrm: 0.396576| num_tokens: 16,277
Step 00424/00701 | Training loss: 0.835709| lrm: 0.395150| num_tokens: 12,681
Step 00425/00701 | Training loss: 0.738783| lrm: 0.393723| num_tokens: 9,047
Step 00426/00701 | Training loss: 1.080087| lrm: 0.392297| num_tokens: 13,775
Step 00427/00701 | Training loss: 0.598845| lrm: 0.390870| num_tokens: 10,690
Step 00428/00701 | Training loss: 1.344167| lrm: 0.389444| num_tokens: 9,404
Step 00429/00701 | Training loss: 0.515757| lrm: 0.388017| num_tokens: 7,569
Step 00430/00701 | Training loss: 0.676629| lrm: 0.386591| num_tokens: 11,801
Step 00431/00701 | Training loss: 0.538559| lrm: 0.385164| num_tokens: 10,107
Step 00432/00701 | Training loss: 0.547164| lrm: 0.383738| num_tokens: 10,651
Step 00433/00701 | Training loss: 1.216939| lrm: 0.382311| num_tokens: 13,779
Step 00434/00701 | Training loss: 0.478133| lrm: 0.380884| num_tokens: 12,334
Step 00435/00701 | Training loss: 0.732800| lrm: 0.379458| num_tokens: 11,174
Step 00436/00701 | Training loss: 0.822928| lrm: 0.378031| num_tokens: 11,492
Step 00437/00701 | Training loss: 0.793927| lrm: 0.376605| num_tokens: 13,126
Step 00438/00701 | Training loss: 1.088126| lrm: 0.375178| num_tokens: 8,901
Step 00439/00701 | Training loss: 0.761771| lrm: 0.373752| num_tokens: 13,642
Step 00440/00701 | Training loss: 0.418554| lrm: 0.372325| num_tokens: 11,015
Step 00441/00701 | Training loss: 0.826542| lrm: 0.370899| num_tokens: 13,052
Step 00442/00701 | Training loss: 0.930416| lrm: 0.369472| num_tokens: 12,739
Step 00443/00701 | Training loss: 0.531891| lrm: 0.368046| num_tokens: 10,479
Step 00444/00701 | Training loss: 1.090953| lrm: 0.366619| num_tokens: 12,465
Step 00445/00701 | Training loss: 0.955371| lrm: 0.365193| num_tokens: 15,012
Step 00446/00701 | Training loss: 1.079784| lrm: 0.363766| num_tokens: 13,188
Step 00447/00701 | Training loss: 1.079857| lrm: 0.362340| num_tokens: 14,154
Step 00448/00701 | Training loss: 0.846269| lrm: 0.360913| num_tokens: 9,629
Step 00449/00701 | Training loss: 0.636252| lrm: 0.359486| num_tokens: 13,155
Step 00450/00701 | Training loss: 1.018172| lrm: 0.358060| num_tokens: 14,412
Step 00451/00701 | Training loss: 1.220985| lrm: 0.356633| num_tokens: 10,075
Step 00452/00701 | Training loss: 0.701327| lrm: 0.355207| num_tokens: 11,930
Step 00453/00701 | Training loss: 1.054636| lrm: 0.353780| num_tokens: 12,863
Step 00454/00701 | Training loss: 0.623713| lrm: 0.352354| num_tokens: 7,038
Step 00455/00701 | Training loss: 0.484764| lrm: 0.350927| num_tokens: 10,003
Step 00456/00701 | Training loss: 0.698672| lrm: 0.349501| num_tokens: 15,261
Step 00457/00701 | Training loss: 0.737039| lrm: 0.348074| num_tokens: 8,720
Step 00458/00701 | Training loss: 0.757050| lrm: 0.346648| num_tokens: 6,493
Step 00459/00701 | Training loss: 0.770791| lrm: 0.345221| num_tokens: 12,606
Step 00460/00701 | Training loss: 0.452051| lrm: 0.343795| num_tokens: 8,519
Step 00461/00701 | Training loss: 1.076383| lrm: 0.342368| num_tokens: 11,465
Step 00462/00701 | Training loss: 0.544653| lrm: 0.340942| num_tokens: 11,227
Step 00463/00701 | Training loss: 0.735209| lrm: 0.339515| num_tokens: 7,042
Step 00464/00701 | Training loss: 0.679686| lrm: 0.338088| num_tokens: 10,781
Step 00465/00701 | Training loss: 0.889199| lrm: 0.336662| num_tokens: 9,106
Step 00466/00701 | Training loss: 1.001840| lrm: 0.335235| num_tokens: 8,984
Step 00467/00701 | Training loss: 1.196598| lrm: 0.333809| num_tokens: 10,821
Step 00468/00701 | Training loss: 0.675088| lrm: 0.332382| num_tokens: 13,818
Step 00469/00701 | Training loss: 0.543271| lrm: 0.330956| num_tokens: 7,782
Step 00470/00701 | Training loss: 0.872125| lrm: 0.329529| num_tokens: 9,190
Step 00471/00701 | Training loss: 0.791014| lrm: 0.328103| num_tokens: 14,577
Step 00472/00701 | Training loss: 0.799358| lrm: 0.326676| num_tokens: 13,242
Step 00473/00701 | Training loss: 0.997233| lrm: 0.325250| num_tokens: 7,088
Step 00474/00701 | Training loss: 1.017695| lrm: 0.323823| num_tokens: 14,341
Step 00475/00701 | Training loss: 0.546150| lrm: 0.322397| num_tokens: 17,055
Step 00476/00701 | Training loss: 0.767501| lrm: 0.320970| num_tokens: 13,036
Step 00477/00701 | Training loss: 0.848684| lrm: 0.319544| num_tokens: 14,172
Step 00478/00701 | Training loss: 0.931295| lrm: 0.318117| num_tokens: 8,969
Step 00479/00701 | Training loss: 0.460745| lrm: 0.316690| num_tokens: 11,879
Step 00480/00701 | Training loss: 1.176878| lrm: 0.315264| num_tokens: 11,947
Step 00481/00701 | Training loss: 0.752652| lrm: 0.313837| num_tokens: 8,490
Step 00482/00701 | Training loss: 0.873910| lrm: 0.312411| num_tokens: 11,201
Step 00483/00701 | Training loss: 0.901917| lrm: 0.310984| num_tokens: 9,803
Step 00484/00701 | Training loss: 0.865719| lrm: 0.309558| num_tokens: 10,858
Step 00485/00701 | Training loss: 1.017626| lrm: 0.308131| num_tokens: 7,538
Step 00486/00701 | Training loss: 0.315354| lrm: 0.306705| num_tokens: 9,455
Step 00487/00701 | Training loss: 1.099507| lrm: 0.305278| num_tokens: 9,675
Step 00488/00701 | Training loss: 0.889559| lrm: 0.303852| num_tokens: 11,548
Step 00489/00701 | Training loss: 0.938468| lrm: 0.302425| num_tokens: 14,982
Step 00490/00701 | Training loss: 0.623393| lrm: 0.300999| num_tokens: 9,448
Step 00491/00701 | Training loss: 0.761698| lrm: 0.299572| num_tokens: 11,806
Step 00492/00701 | Training loss: 0.474515| lrm: 0.298146| num_tokens: 12,643
Step 00493/00701 | Training loss: 0.332861| lrm: 0.296719| num_tokens: 4,752
Step 00494/00701 | Training loss: 0.936056| lrm: 0.295292| num_tokens: 9,835
Step 00495/00701 | Training loss: 0.830243| lrm: 0.293866| num_tokens: 10,903
Step 00496/00701 | Training loss: 1.041928| lrm: 0.292439| num_tokens: 9,607
Step 00497/00701 | Training loss: 1.172578| lrm: 0.291013| num_tokens: 11,915
Step 00498/00701 | Training loss: 1.184399| lrm: 0.289586| num_tokens: 15,932
Step 00499/00701 | Training loss: 0.906200| lrm: 0.288160| num_tokens: 17,556
Step 00500 | Validation loss: 1.020124
Step 00500/00701 | Training loss: 0.634368| lrm: 0.286733| num_tokens: 14,360
Step 00501/00701 | Training loss: 1.120690| lrm: 0.285307| num_tokens: 8,077
Step 00502/00701 | Training loss: 1.047037| lrm: 0.283880| num_tokens: 8,647
Step 00503/00701 | Training loss: 1.099006| lrm: 0.282454| num_tokens: 12,356
Step 00504/00701 | Training loss: 0.889620| lrm: 0.281027| num_tokens: 11,936
Step 00505/00701 | Training loss: 0.626290| lrm: 0.279601| num_tokens: 8,677
Step 00506/00701 | Training loss: 0.675037| lrm: 0.278174| num_tokens: 8,364
Step 00507/00701 | Training loss: 0.675830| lrm: 0.276748| num_tokens: 5,963
Step 00508/00701 | Training loss: 0.832265| lrm: 0.275321| num_tokens: 13,486
Step 00509/00701 | Training loss: 0.761809| lrm: 0.273894| num_tokens: 9,320
Step 00510/00701 | Training loss: 0.707266| lrm: 0.272468| num_tokens: 9,944
Step 00511/00701 | Training loss: 0.769316| lrm: 0.271041| num_tokens: 9,068
Step 00512/00701 | Training loss: 0.741633| lrm: 0.269615| num_tokens: 11,839
Step 00513/00701 | Training loss: 0.910943| lrm: 0.268188| num_tokens: 9,382
Step 00514/00701 | Training loss: 0.834989| lrm: 0.266762| num_tokens: 10,529
Step 00515/00701 | Training loss: 0.900670| lrm: 0.265335| num_tokens: 12,058
Step 00516/00701 | Training loss: 0.988708| lrm: 0.263909| num_tokens: 14,512
Step 00517/00701 | Training loss: 0.906147| lrm: 0.262482| num_tokens: 10,646
Step 00518/00701 | Training loss: 0.880920| lrm: 0.261056| num_tokens: 12,085
Step 00519/00701 | Training loss: 0.585556| lrm: 0.259629| num_tokens: 7,230
Step 00520/00701 | Training loss: 0.773524| lrm: 0.258203| num_tokens: 10,873
Step 00521/00701 | Training loss: 0.509819| lrm: 0.256776| num_tokens: 13,475
Step 00522/00701 | Training loss: 0.930181| lrm: 0.255350| num_tokens: 12,653
Step 00523/00701 | Training loss: 0.652859| lrm: 0.253923| num_tokens: 11,566
Step 00524/00701 | Training loss: 0.945771| lrm: 0.252496| num_tokens: 5,489
Step 00525/00701 | Training loss: 0.906439| lrm: 0.251070| num_tokens: 14,450
Step 00526/00701 | Training loss: 0.717001| lrm: 0.249643| num_tokens: 9,476
Step 00527/00701 | Training loss: 0.839083| lrm: 0.248217| num_tokens: 8,879
Step 00528/00701 | Training loss: 0.991755| lrm: 0.246790| num_tokens: 13,237
Step 00529/00701 | Training loss: 0.940010| lrm: 0.245364| num_tokens: 15,651
Step 00530/00701 | Training loss: 0.921672| lrm: 0.243937| num_tokens: 11,639
Step 00531/00701 | Training loss: 0.743235| lrm: 0.242511| num_tokens: 8,225
Step 00532/00701 | Training loss: 0.671795| lrm: 0.241084| num_tokens: 7,667
Step 00533/00701 | Training loss: 0.854511| lrm: 0.239658| num_tokens: 10,695
Step 00534/00701 | Training loss: 0.704148| lrm: 0.238231| num_tokens: 10,511
Step 00535/00701 | Training loss: 0.529026| lrm: 0.236805| num_tokens: 8,932
Step 00536/00701 | Training loss: 0.744502| lrm: 0.235378| num_tokens: 10,329
Step 00537/00701 | Training loss: 0.508164| lrm: 0.233951| num_tokens: 10,695
Step 00538/00701 | Training loss: 0.885057| lrm: 0.232525| num_tokens: 11,001
Step 00539/00701 | Training loss: 0.841079| lrm: 0.231098| num_tokens: 10,673
Step 00540/00701 | Training loss: 0.357172| lrm: 0.229672| num_tokens: 9,119
Step 00541/00701 | Training loss: 0.987966| lrm: 0.228245| num_tokens: 19,235
Step 00542/00701 | Training loss: 0.538206| lrm: 0.226819| num_tokens: 9,908
Step 00543/00701 | Training loss: 1.194140| lrm: 0.225392| num_tokens: 12,841
Step 00544/00701 | Training loss: 0.572154| lrm: 0.223966| num_tokens: 9,921
Step 00545/00701 | Training loss: 0.713854| lrm: 0.222539| num_tokens: 13,247
Step 00546/00701 | Training loss: 0.956191| lrm: 0.221113| num_tokens: 14,587
Step 00547/00701 | Training loss: 0.636897| lrm: 0.219686| num_tokens: 7,478
Step 00548/00701 | Training loss: 0.637313| lrm: 0.218260| num_tokens: 13,789
Step 00549/00701 | Training loss: 0.784875| lrm: 0.216833| num_tokens: 8,768
Step 00550/00701 | Training loss: 0.404255| lrm: 0.215407| num_tokens: 11,010
Step 00551/00701 | Training loss: 0.720909| lrm: 0.213980| num_tokens: 9,295
Step 00552/00701 | Training loss: 1.089764| lrm: 0.212553| num_tokens: 10,564
Step 00553/00701 | Training loss: 0.738940| lrm: 0.211127| num_tokens: 8,781
Step 00554/00701 | Training loss: 1.031889| lrm: 0.209700| num_tokens: 12,788
Step 00555/00701 | Training loss: 0.963466| lrm: 0.208274| num_tokens: 9,962
Step 00556/00701 | Training loss: 0.572255| lrm: 0.206847| num_tokens: 7,612
Step 00557/00701 | Training loss: 0.939533| lrm: 0.205421| num_tokens: 8,984
Step 00558/00701 | Training loss: 1.147326| lrm: 0.203994| num_tokens: 9,789
Step 00559/00701 | Training loss: 0.798368| lrm: 0.202568| num_tokens: 9,455
Step 00560/00701 | Training loss: 0.976264| lrm: 0.201141| num_tokens: 13,741
Step 00561/00701 | Training loss: 0.763888| lrm: 0.199715| num_tokens: 6,445
Step 00562/00701 | Training loss: 0.949834| lrm: 0.198288| num_tokens: 10,949
Step 00563/00701 | Training loss: 1.090007| lrm: 0.196862| num_tokens: 10,860
Step 00564/00701 | Training loss: 1.202555| lrm: 0.195435| num_tokens: 11,574
Step 00565/00701 | Training loss: 0.968855| lrm: 0.194009| num_tokens: 13,118
Step 00566/00701 | Training loss: 0.775584| lrm: 0.192582| num_tokens: 11,206
Step 00567/00701 | Training loss: 0.742776| lrm: 0.191155| num_tokens: 17,559
Step 00568/00701 | Training loss: 1.506338| lrm: 0.189729| num_tokens: 10,194
Step 00569/00701 | Training loss: 0.774501| lrm: 0.188302| num_tokens: 10,146
Step 00570/00701 | Training loss: 0.698195| lrm: 0.186876| num_tokens: 9,911
Step 00571/00701 | Training loss: 0.784079| lrm: 0.185449| num_tokens: 15,273
Step 00572/00701 | Training loss: 1.533888| lrm: 0.184023| num_tokens: 9,168
Step 00573/00701 | Training loss: 0.681524| lrm: 0.182596| num_tokens: 10,513
Step 00574/00701 | Training loss: 0.578842| lrm: 0.181170| num_tokens: 13,029
Step 00575/00701 | Training loss: 0.881109| lrm: 0.179743| num_tokens: 7,137
Step 00576/00701 | Training loss: 0.557514| lrm: 0.178317| num_tokens: 11,369
Step 00577/00701 | Training loss: 1.344787| lrm: 0.176890| num_tokens: 11,506
Step 00578/00701 | Training loss: 0.850237| lrm: 0.175464| num_tokens: 7,429
Step 00579/00701 | Training loss: 0.926690| lrm: 0.174037| num_tokens: 13,683
Step 00580/00701 | Training loss: 0.953827| lrm: 0.172611| num_tokens: 9,031
Step 00581/00701 | Training loss: 0.587835| lrm: 0.171184| num_tokens: 13,406
Step 00582/00701 | Training loss: 0.625518| lrm: 0.169757| num_tokens: 9,951
Step 00583/00701 | Training loss: 0.832450| lrm: 0.168331| num_tokens: 11,941
Step 00584/00701 | Training loss: 0.934648| lrm: 0.166904| num_tokens: 10,528
Step 00585/00701 | Training loss: 0.877052| lrm: 0.165478| num_tokens: 11,897
Step 00586/00701 | Training loss: 1.124688| lrm: 0.164051| num_tokens: 10,330
Step 00587/00701 | Training loss: 0.586349| lrm: 0.162625| num_tokens: 9,390
Step 00588/00701 | Training loss: 1.009011| lrm: 0.161198| num_tokens: 8,863
Step 00589/00701 | Training loss: 1.200030| lrm: 0.159772| num_tokens: 8,837
Step 00590/00701 | Training loss: 1.158050| lrm: 0.158345| num_tokens: 11,323
Step 00591/00701 | Training loss: 1.011073| lrm: 0.156919| num_tokens: 8,399
Step 00592/00701 | Training loss: 1.032400| lrm: 0.155492| num_tokens: 10,391
Step 00593/00701 | Training loss: 1.141144| lrm: 0.154066| num_tokens: 10,713
Step 00594/00701 | Training loss: 0.553435| lrm: 0.152639| num_tokens: 11,467
Step 00595/00701 | Training loss: 1.013099| lrm: 0.151213| num_tokens: 8,244
Step 00596/00701 | Training loss: 1.314370| lrm: 0.149786| num_tokens: 6,168
Step 00597/00701 | Training loss: 0.466862| lrm: 0.148359| num_tokens: 10,601
Step 00598/00701 | Training loss: 0.790209| lrm: 0.146933| num_tokens: 8,123
Step 00599/00701 | Training loss: 1.005761| lrm: 0.145506| num_tokens: 8,163
Step 00600 | Validation loss: 1.019713
Final: 349/1024 (34.08%)
Final: 451/1024 (44.04%)
Step 00600 | mmlu_acc: 0.340820, arc_easy_acc: 0.440430
Step 00600/00701 | Training loss: 0.701532| lrm: 0.144080| num_tokens: 11,170
Step 00601/00701 | Training loss: 1.087604| lrm: 0.142653| num_tokens: 10,313
Step 00602/00701 | Training loss: 1.006274| lrm: 0.141227| num_tokens: 8,582
Step 00603/00701 | Training loss: 0.509402| lrm: 0.139800| num_tokens: 9,536
Step 00604/00701 | Training loss: 0.936537| lrm: 0.138374| num_tokens: 11,592
Step 00605/00701 | Training loss: 1.261975| lrm: 0.136947| num_tokens: 10,561
Step 00606/00701 | Training loss: 0.856510| lrm: 0.135521| num_tokens: 9,894
Step 00607/00701 | Training loss: 1.082292| lrm: 0.134094| num_tokens: 15,789
Step 00608/00701 | Training loss: 0.758863| lrm: 0.132668| num_tokens: 13,347
Step 00609/00701 | Training loss: 0.554357| lrm: 0.131241| num_tokens: 10,958
Step 00610/00701 | Training loss: 1.036474| lrm: 0.129815| num_tokens: 15,107
Step 00611/00701 | Training loss: 0.831206| lrm: 0.128388| num_tokens: 11,298
Step 00612/00701 | Training loss: 0.991855| lrm: 0.126961| num_tokens: 9,848
Step 00613/00701 | Training loss: 0.914285| lrm: 0.125535| num_tokens: 15,923
Step 00614/00701 | Training loss: 0.819444| lrm: 0.124108| num_tokens: 13,977
Step 00615/00701 | Training loss: 1.293125| lrm: 0.122682| num_tokens: 10,275
Step 00616/00701 | Training loss: 0.935231| lrm: 0.121255| num_tokens: 9,826
Step 00617/00701 | Training loss: 0.970106| lrm: 0.119829| num_tokens: 10,290
Step 00618/00701 | Training loss: 0.539848| lrm: 0.118402| num_tokens: 8,782
Step 00619/00701 | Training loss: 1.099248| lrm: 0.116976| num_tokens: 8,824
Step 00620/00701 | Training loss: 1.062591| lrm: 0.115549| num_tokens: 12,228
Step 00621/00701 | Training loss: 0.999978| lrm: 0.114123| num_tokens: 8,977
Step 00622/00701 | Training loss: 0.636187| lrm: 0.112696| num_tokens: 6,748
Step 00623/00701 | Training loss: 0.772975| lrm: 0.111270| num_tokens: 13,482
Step 00624/00701 | Training loss: 0.979816| lrm: 0.109843| num_tokens: 11,419
Step 00625/00701 | Training loss: 1.194508| lrm: 0.108417| num_tokens: 9,960
Step 00626/00701 | Training loss: 0.986552| lrm: 0.106990| num_tokens: 10,377
Step 00627/00701 | Training loss: 0.948617| lrm: 0.105563| num_tokens: 8,388
Step 00628/00701 | Training loss: 1.032453| lrm: 0.104137| num_tokens: 9,380
Step 00629/00701 | Training loss: 1.218420| lrm: 0.102710| num_tokens: 11,667
Step 00630/00701 | Training loss: 1.013900| lrm: 0.101284| num_tokens: 11,189
Step 00631/00701 | Training loss: 1.087871| lrm: 0.099857| num_tokens: 11,108
Step 00632/00701 | Training loss: 0.995815| lrm: 0.098431| num_tokens: 11,425
Step 00633/00701 | Training loss: 1.041842| lrm: 0.097004| num_tokens: 10,780
Step 00634/00701 | Training loss: 0.742178| lrm: 0.095578| num_tokens: 8,335
Step 00635/00701 | Training loss: 0.627768| lrm: 0.094151| num_tokens: 12,937
Step 00636/00701 | Training loss: 1.608795| lrm: 0.092725| num_tokens: 8,767
Step 00637/00701 | Training loss: 1.113993| lrm: 0.091298| num_tokens: 14,092
Step 00638/00701 | Training loss: 1.110543| lrm: 0.089872| num_tokens: 9,224
Step 00639/00701 | Training loss: 0.743269| lrm: 0.088445| num_tokens: 12,485
Step 00640/00701 | Training loss: 1.078288| lrm: 0.087019| num_tokens: 12,952
Step 00641/00701 | Training loss: 1.266070| lrm: 0.085592| num_tokens: 9,450
Step 00642/00701 | Training loss: 1.242364| lrm: 0.084165| num_tokens: 12,985
Step 00643/00701 | Training loss: 0.838491| lrm: 0.082739| num_tokens: 8,890
Step 00644/00701 | Training loss: 1.343226| lrm: 0.081312| num_tokens: 8,515
Step 00645/00701 | Training loss: 1.206944| lrm: 0.079886| num_tokens: 14,196
Step 00646/00701 | Training loss: 0.574244| lrm: 0.078459| num_tokens: 13,224
Step 00647/00701 | Training loss: 1.289369| lrm: 0.077033| num_tokens: 9,157
Step 00648/00701 | Training loss: 0.711776| lrm: 0.075606| num_tokens: 9,040
Step 00649/00701 | Training loss: 0.719556| lrm: 0.074180| num_tokens: 14,037
Step 00650/00701 | Training loss: 0.760805| lrm: 0.072753| num_tokens: 5,963
Step 00651/00701 | Training loss: 0.960547| lrm: 0.071327| num_tokens: 15,333
Step 00652/00701 | Training loss: 0.978859| lrm: 0.069900| num_tokens: 10,486
Step 00653/00701 | Training loss: 1.029290| lrm: 0.068474| num_tokens: 8,748
Step 00654/00701 | Training loss: 1.029676| lrm: 0.067047| num_tokens: 10,346
Step 00655/00701 | Training loss: 0.410298| lrm: 0.065621| num_tokens: 11,278
Step 00656/00701 | Training loss: 0.659129| lrm: 0.064194| num_tokens: 10,805
Step 00657/00701 | Training loss: 1.012633| lrm: 0.062767| num_tokens: 10,353
Step 00658/00701 | Training loss: 0.726918| lrm: 0.061341| num_tokens: 10,027
Step 00659/00701 | Training loss: 0.420437| lrm: 0.059914| num_tokens: 11,324
Step 00660/00701 | Training loss: 1.122667| lrm: 0.058488| num_tokens: 8,166
Step 00661/00701 | Training loss: 0.763163| lrm: 0.057061| num_tokens: 11,277
Step 00662/00701 | Training loss: 1.056241| lrm: 0.055635| num_tokens: 8,088
Step 00663/00701 | Training loss: 0.990940| lrm: 0.054208| num_tokens: 9,365
Step 00664/00701 | Training loss: 0.641331| lrm: 0.052782| num_tokens: 10,132
Step 00665/00701 | Training loss: 0.679602| lrm: 0.051355| num_tokens: 9,223
Step 00666/00701 | Training loss: 1.069211| lrm: 0.049929| num_tokens: 12,028
Step 00667/00701 | Training loss: 0.746847| lrm: 0.048502| num_tokens: 12,278
Step 00668/00701 | Training loss: 0.926038| lrm: 0.047076| num_tokens: 13,148
Step 00669/00701 | Training loss: 0.898869| lrm: 0.045649| num_tokens: 9,244
Step 00670/00701 | Training loss: 0.833983| lrm: 0.044223| num_tokens: 11,272
Step 00671/00701 | Training loss: 0.682537| lrm: 0.042796| num_tokens: 9,958
Step 00672/00701 | Training loss: 1.146627| lrm: 0.041369| num_tokens: 11,635
Step 00673/00701 | Training loss: 1.011553| lrm: 0.039943| num_tokens: 9,736
Step 00674/00701 | Training loss: 0.671581| lrm: 0.038516| num_tokens: 9,781
Step 00675/00701 | Training loss: 0.524962| lrm: 0.037090| num_tokens: 10,475
Step 00676/00701 | Training loss: 0.694858| lrm: 0.035663| num_tokens: 9,690
Step 00677/00701 | Training loss: 0.828047| lrm: 0.034237| num_tokens: 16,501
Step 00678/00701 | Training loss: 0.896392| lrm: 0.032810| num_tokens: 12,799
Step 00679/00701 | Training loss: 0.800295| lrm: 0.031384| num_tokens: 7,782
Step 00680/00701 | Training loss: 0.934515| lrm: 0.029957| num_tokens: 14,144
Step 00681/00701 | Training loss: 1.072741| lrm: 0.028531| num_tokens: 8,384
Step 00682/00701 | Training loss: 0.866917| lrm: 0.027104| num_tokens: 14,386
Step 00683/00701 | Training loss: 0.985253| lrm: 0.025678| num_tokens: 9,148
Step 00684/00701 | Training loss: 1.052458| lrm: 0.024251| num_tokens: 12,728
Step 00685/00701 | Training loss: 0.860402| lrm: 0.022825| num_tokens: 10,770
Step 00686/00701 | Training loss: 0.984616| lrm: 0.021398| num_tokens: 9,747
Step 00687/00701 | Training loss: 0.755424| lrm: 0.019971| num_tokens: 12,622
Step 00688/00701 | Training loss: 0.848354| lrm: 0.018545| num_tokens: 11,487
Step 00689/00701 | Training loss: 0.641019| lrm: 0.017118| num_tokens: 9,652
Step 00690/00701 | Training loss: 0.865964| lrm: 0.015692| num_tokens: 9,456
Step 00691/00701 | Training loss: 0.966582| lrm: 0.014265| num_tokens: 10,567
Step 00692/00701 | Training loss: 0.804496| lrm: 0.012839| num_tokens: 12,148
Step 00693/00701 | Training loss: 0.892150| lrm: 0.011412| num_tokens: 11,030
Step 00694/00701 | Training loss: 0.024548| lrm: 0.009986| num_tokens: 10,060
Step 00695/00701 | Training loss: 0.831299| lrm: 0.008559| num_tokens: 7,699
Step 00696/00701 | Training loss: 1.008993| lrm: 0.007133| num_tokens: 12,939
Step 00697/00701 | Training loss: 0.818547| lrm: 0.005706| num_tokens: 10,980
Step 00698/00701 | Training loss: 0.800103| lrm: 0.004280| num_tokens: 9,928
Step 00699/00701 | Training loss: 0.781520| lrm: 0.002853| num_tokens: 10,009
Step 00700 | Validation loss: 1.019758
Final: 356/1024 (34.77%)
Final: 452/1024 (44.14%)
Step 00700 | mmlu_acc: 0.347656, arc_easy_acc: 0.441406
2025-11-30 21:02:50,786 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/ubuntu/.cache/nanochat/chatsft_checkpoints/d20/model_000700.pt
2025-11-30 21:02:50,787 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/ubuntu/.cache/nanochat/chatsft_checkpoints/d20/meta_000700.json
‚úÖ Saved model checkpoint to /home/ubuntu/.cache/nanochat/chatsft_checkpoints/d20
